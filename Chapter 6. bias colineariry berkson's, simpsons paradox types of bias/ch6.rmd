---
title: 'Chapter 6'
output: github_document
---

<!--   output: -->
<!--   html_document: -->
<!--     df_print: paged -->

<!-- editor_options: -->
<!--   chunk_output_type: inline -->

```{r}
suppressMessages(library(rethinking))
suppressMessages(library(magrittr))
```

### Berkson's Paradox/selection distortion effect, multicolinearity, non-identifiability, omitted variable bias, post-treatment bias, D-separation, conditional independences, simpson's paradox, confounding, backdoor blocking confounding paths.  

This chapter introduces the 4 elemental confounds, pipe, fork, collider, descendent and how they influence regression through simulation, then how to correct for them with DAG analysis. This all asumes you know have some hypothesis about the direction of association for every outcome you are testing. If you had 10K outcome variables like genes, this approach would require you to either assume the same causal structure (implied by adding covariates in a model applied to all genes) or create a saparate DAG for each outcome variable. The simulations are useful for getting intuition on how confounding happens inside a multiple regression problem.  


Berkson's Paradox/selection distortion effect/ collider bias:  
Strong selection induces negative correlation in the variables used for selection. Here simulating grant proposal selection based on 2 criteria, trustrowthiness and newsworthiness. When you select based on both variables simultaneously, the 2 varlables which are independent  **become conditionally dependent and negatively correlated**.  


```{r}
## R code 6.1
set.seed(1914)
N <- 200 # num grant proposals
p <- 0.1 # proportion to select
# uncorrelated newsworthiness and trustworthiness
nw <- rnorm(N)
tw <- rnorm(N)
precis(lm(tw~nw))

# select top 10% of combined scores
s <- nw + tw  # total score
q <- quantile( s , 1-p ) # top 10% threshold
selected <- ifelse( s >= q , TRUE , FALSE )
cor( tw[selected] , nw[selected] )
```

## 6.1 Multicolinearity  

The correlation between 2 variables; colinearity betwen predictors conditional on other variables in the model is most important, rather than just raw correlation between variables.  

Demonstration: correlation between height and leg length - what happens if we include left length + right length in the model:  


```{r}
## R code 6.2
# number of individuals
N <- 100                  
set.seed(909)
# sim total height of each
height <- rnorm(N,10,2)           

# leg as proportion of height
leg_prop <- runif(N,0.4,0.5)      


# sim left leg as proportion + error
leg_left <- leg_prop*height + rnorm( N , 0 , 0.02 )

# sim right leg as proportion + error
leg_right <- leg_prop*height + rnorm( N , 0 , 0.02 )    
   
# combine into data frame
d <- data.frame(height,leg_left,leg_right)

## R code 6.3
m6.1 <- quap(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + bl*leg_left + br*leg_right ,
        a ~ dnorm( 10 , 100 ) ,
        bl ~ dnorm( 2 , 10 ) ,
        br ~ dnorm( 2 , 10 ) ,
        sigma ~ dexp( 1 )
    ) , data=d )

plot(precis(m6.1))


```

here we are asking what is the value of knowing each predictor after knowing all other predictors. What is the value of knowing left leg height after knowing right leg height?  

The posterior counts the relative plausibilities of every possible combination of parameter values conditional on model + data.  
The posterior for the left leg effect and right leg effect are highly negatively correlated in a narrow range of possible values in the posterior. If right leg effect is large, the left leg effect must be very small -- this is obviously getting the causal story wrong. 

```{r}
## R code 6.5
post <- extract.samples(m6.1)
post %>% str()
plot( bl ~ br , post , col=col.alpha(rangi2,0.1) , pch=16 )

```

Since b1[x1] and b2[x2] are measuring the same thing, its like our model is using the same predictor twice and doing b1[x1] + b2[x1], so that equates to (b1 + b2)x1. Only the sum of b1 and b2 influence the outcome variable. If we extract the sum of the 2 betas from the posterior, we actually get the correct estimate:  

```{r}
## R code 6.6
sum_blbr <- post$bl + post$br
dens( sum_blbr , col=rangi2 , lwd=2 , xlab="sum of bl and br" )

```


```{r}

# now fit model with just 1 leg length 
## R code 6.7
m6.2 <- quap(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + bl*leg_left,
        a ~ dnorm( 10 , 100 ) ,
        bl ~ dnorm( 2 , 10 ) ,
        sigma ~ dexp( 1 )
    ) , data=d )
precis(m6.2)
```


Note the incorrect model makes fine predictions it is just not interpretable.  

Another example from the monkey milk dataset. 

When we include 2 highly colinear variables that contain the same information, the effect of each in a multiple regression is diminished highly with increased s.d. compared to the single variable regression. This is for the same reason as aobve, we are only estimating the sum of the effects. *Neither variable one helps as much once you know the other.*  Ideally we should not just drop highly colinear variables by default; in order to know what do do we must know the conditional independences which can only be inferred by the DAG structure. 

```{r}
## R code 6.8

data(milk)
d <- milk
d$K <- standardize( d$kcal.per.g )
d$F <- standardize( d$perc.fat )
d$L <- standardize( d$perc.lactose )

## R code 6.9
# kcal.per.g regressed on perc.fat
m6.3 <- quap(
    alist(
        K ~ dnorm( mu , sigma ) ,
        mu <- a + bF*F,
        a ~ dnorm( 0 , 0.2 ) ,
        bF ~ dnorm( 0 , 0.5 ) ,
        sigma ~ dexp( 1 )
    ) , data=d )

# kcal.per.g regressed on perc.lactose
m6.4 <- quap(
    alist(
        K ~ dnorm( mu , sigma ) ,
        mu <- a + bL*L ,
        a ~ dnorm( 0 , 0.2 ) ,
        bL ~ dnorm( 0 , 0.5 ) ,
        sigma ~ dexp( 1 )
    ) , data=d )


## R code 6.10
m6.5 <- quap(
    alist(
        K ~ dnorm( mu , sigma ) ,
        mu <- a + bF*F + bL*L ,
        a ~ dnorm( 0 , 0.2 ) ,
        bF ~ dnorm( 0 , 0.5 ) ,
        bL ~ dnorm( 0 , 0.5 ) ,
        sigma ~ dexp( 1 )
    ) ,
    data=d )




plot(precis( m6.3 ), main = 'm6.3')
plot(precis( m6.4 ), main = 'm6.4')
plot(precis( m6.5 ), main = 'm6.5')
```

```{r}
## R code 6.11
pairs( ~ kcal.per.g + perc.fat + perc.lactose , data=d , col=rangi2 )

```

In this simple dataset there is a nice anthro life history interpretation which can give us a hypothesis abotu the potential causal structure. There is a tradeoff in milk composition high nursing *frequency* reduces fat content and increases sugar content, reducing overall milk energy. An evolutionary / economic model would be better than forcing lines through the data in this case. This is an example of *non-identifiability* the structure of the data and model do not make it possible to estimate a parameters value technically we do estimate values since the model integrates to 1 but the interpretation is not clear. 

How to simulate correlated predictors and view how the imprecision (sd) of the posterior increases with increasing association between 2 predictors. 
```{r}
## R code 6.12
d <- milk

sim.coll <- function( r ) {
    d$x <- rnorm(n =  nrow(d), mean = r*d$perc.fat, sd = sqrt((1 - r^2) * var(d$perc.fat)))
    m <- lm( kcal.per.g ~ perc.fat + x , data=d )
    sqrt(diag(vcov(m)))[2] # stddev of parameter
}
# function that returns the standard deviation of the slope relating perc.fat to kcal milk.
rep.sim.coll <- function(r, n) {
    stddev <- replicate(n = n, expr = sim.coll(r))
    return(mean(stddev))
}
r.seq <- seq(from=0,to=0.99,by=0.1)
stddev <- sapply(r.seq ,
                 function(z){  
                     rep.sim.coll(r=z,n=100) }
                 )
plot( stddev ~ r.seq , type="l" , col=rangi2, lwd=2 ,
      xlab = "simulated correlation between simulated variable x and perc. fat", 
      ylab = "imprecision (s.d.) of perc.fat effect" )

```


### Post treatment bias - a type of bias from including a confounder  

In this example, plant growth effect h0 to h1 if impacted by a treatment which exerts its effect by inhibiting fungal growth. **the imulation explicitely makes treatment influence growth.**

```{r}
## R code 6.13
set.seed(71)
# number of plants
N <- 500

# simulate initial heights
h0 <- rnorm(n = N,mean = 10,sd = 2)

# assign treatments and simulate fungus and growth
treatment <- rep(0:1, each = N/2)
# the probability of being infected is 0.5 if plant didn't get the treatment 
# and 0.1 if the plant DID get the treatment. 
fungus <- rbinom(n = N, size = 1, prob = (0.5 - treatment*0.4) )

# height after the experiment 
h1 <- h0 + rnorm(N, 5 - 3*fungus)

# compose a clean data frame
d <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )
precis(d)
```

For the model, we model the proportion of height change after the experiment. p = 2 means doubling in height. WE can have negative growth meaning the plants died. The prior sampling shows we expect 40% decrease to 50% increase in growth. Kind of weird since plant death does not reduce growth, it would be better to have some indicator for death or something. 

```{r}
## R code 6.14
sim_p <- rlnorm( 1e4 , 0 , 0.25 )
precis( data.frame(sim_p) )

## R code 6.15
m6.6 <- quap(
    alist(
        h1 ~ dnorm( mu , sigma ),
        mu <- h0*p,
        p ~ dlnorm( 0 , 0.25 ),
        sigma ~ dexp( 1 )
    ), data=d )
precis(m6.6)
```

This posterior of the average growth in the experiment means about 40% (proportion = 1.39) growth. 

Fit multiple regression model  inclusing treatment and fungus (which was observed *after* the experiment was finished). The treatment now has 0 effect!  

```{r}

## R code 6.16
m6.7 <- quap(
    alist(
        h1 ~ dnorm( mu , sigma ),
        mu <- h0 * p,
        p <- a + bt*treatment + bf*fungus,
        a ~ dlnorm( 0 , 0.2 ) ,
        bt ~ dnorm( 0 , 0.5 ),
        bf ~ dnorm( 0 , 0.5 ),
        sigma ~ dexp( 1 )
    ), data=d )
precis(m6.7)


```

Again, once we already know about the association between fungus and growth, does knowing anything about treatment help us in the prediction. No because mediates effects of growth through reducing fungus; but we wanted to model and understand the treatment effect. Here including post treatment variable F blocked the effect of treatment. Next omit post treatment variable. 


```{r}
## R code 6.17
m6.8 <- quap(
    alist(
        h1 ~ dnorm( mu , sigma ),
        mu <- h0 * p,
        p <- a + bt*treatment,
        a ~ dlnorm( 0 , 0.2 ),
        bt ~ dnorm( 0 , 0.5 ),
        sigma ~ dexp( 1 )
    ), data=d )
precis(m6.8)
```

It is helpful that including fungus zeros the coefficient for treatment because it suggests the mediation effect. Here is another way of thinking about it with the redictor residual plots from ch 5.  

```{r}

d$growth_change = d$h1 - d$h0
d %>% head 

treatment_resid = residuals(lm(treatment ~ fungus, data = d))
m1 = lm(growth_change ~ treatment_resid, data = d)
m2 = lm(growth_change ~ treatment, data = d)

plot(growth_change ~ treatment_resid, data = d, main = 'blue line = no association', 
     xlab = 'treatment residual (regressed on fungus)')
abline(m1, col = "blue")


plot(growth_change ~ treatment, data = d,
     main = 'red line  = treatment effect')
abline(m2, col = "red")
```

This helps uns understand that treatment is mediating its effects through fungus.  

Conditioning on F blocks the path to T (T is *d separated* from T when we condition on F ) 

```{r}
## R code 6.18
library(dagitty)
plant_dag <- dagitty( "dag {
    H_0 -> H_1
    F -> H_1
    T -> F
}")
coordinates( plant_dag ) <- list( x=c(H_0=0,T=2,F=1.5,H_1=1) ,
                                  y=c(H_0=0,T=0,F=0,H_1=0) )
drawdag( plant_dag )
```

```{r}
## R code 6.19
impliedConditionalIndependencies(plant_dag)

```

Assume fungus had NOTHING to do with growth. instead the correct DAG structure is that moisture influences both F and H1 but this different type of fungus again doesnt impact growth. If we condition on F, we introduce **Collider bias** because F is a collider. There is no association between F 

```{r}
new_dag <- dagitty( 
"dag {
    H_0 -> H_1
    M -> H_1
    M -> F
    T -> F}"
    )

drawdag( new_dag )
```

Simulate this in `d2` and re-fit models
```{r}

## R code 6.20
set.seed(71)
N <- 1000
h0 <- rnorm(N,10,2)
treatment <- rep( 0:1 , each=N/2 )
M <- rbern(N)
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 + 0.4*M )
h1 <- h0 + rnorm( N , 5 + 3*M )
d2 <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )


## R code 6.16
m6.7.1 <- quap(
    alist(
        h1 ~ dnorm( mu , sigma ),
        mu <- h0 * p,
        p <- a + bt*treatment + bf*fungus,
        a ~ dlnorm( 0 , 0.2 ) ,
        bt ~ dnorm( 0 , 0.5 ),
        bf ~ dnorm( 0 , 0.5 ),
        sigma ~ dexp( 1 )
    ), data=d2 )
precis(m6.7.1)
## R code 6.17
m6.8.1 <- quap(
    alist(
        h1 ~ dnorm( mu , sigma ),
        mu <- h0 * p,
        p <- a + bt*treatment,
        a ~ dlnorm( 0 , 0.2 ),
        bt ~ dnorm( 0 , 0.5 ),
        sigma ~ dexp( 1 )
    ), data=d2)
precis(m6.8.1)

```

Treatment has no effect but fungus looks like it has a positive effect 


Another example. Happiness H as a function of age A either conditioning or not on marriage M. WE simulate the following causal model with a dynamical model:  


```{r}
h_dag <- dagitty( 
"dag {
    H -> M
    A -> M}"
    )

drawdag( h_dag )
```


simulate each year, 20 people born w/ uniformly distributed happiness. each year happiness does not change. at age 18 people can start getting married and odds of marriage proportional to happiness, no divorce, and after 65 individuals leave the sample. 
```{r}

## R code 6.21
d <- sim_happiness( seed=1977 , N_years=1000 )
precis(d)
```

If we assume that marriage status is a confound and we want to "control for marriage"  
µ[i] = alpha[married_index] + bA*A  

```{r}

## R code 6.22 rescale variables 
d2 <- d[ d$age>17 , ] # only adults
d2$A <- ( d2$age - 18 ) / ( 65 - 18 )

d2[sample( rownames(d2), size = 5), ] 
```

```{r}
## R code 6.23
# specify index variable for marriage status 


# multiple regresion model 
d2$mid <- d2$married + 1
m6.9 <- quap(
    alist(
        happiness ~ dnorm( mu , sigma ),
        mu <- a[mid] + bA*A,
        a[mid] ~ dnorm( 0 , 1 ),
        bA ~ dnorm( 0 , 2 ),
        sigma ~ dexp(1)
    ) , data=d2 )
plot(precis(m6.9,depth=2), main = 'H ~ A + M ')

# only including age not marriage status 
## R code 6.24
m6.10 <- quap(
    alist(
        happiness ~ dnorm( mu , sigma ),
        mu <- a + bA*A,
        a ~ dnorm( 0 , 1 ),
        bA ~ dnorm( 0 , 2 ),
        sigma ~ dexp(1)
    ) , data=d2 )
plot(precis(m6.10), main = 'H ~ A')
```



Intuition:  

Marriage status is a collider in the DAG. It is caused by the predictor (A) and the outcome (H). when we condition on a collider (M), we **induce an association** between both of the things that cause the collider (A and H; here we induce a negative correlation).  

In this model: once we know someone is married or not, their age does not provide information on how happy they are (given the simulation). 


### 6.3.2 unmeasured causes can also create collider bias:  

Grandparents G parents P and children C causal diagram for educational attainment in the presence of an **unmeasured covariate** N(Unmeasured), which represents Neighborhood.  

```{r}

education_dag <- dagitty( 
"dag {
   G -> P 
   G -> C
   P -> C
   NUnmeasured -> P
   NUnmeasured -> C
}"
    )
set.seed(1)
drawdag( education_dag , goodarrow = TRUE )

```

In the simulation below *WE MAKE THE EFFECT G -> C  == 0 and the effect of the 'unmeasured' == 2*
```{r}
## R code 6.25
N <- 200  # number of grandparent-parent-child triads
b_GP <- 1 # direct effect of G on P
b_GC <- 0 # direct effect of G on C
b_PC <- 1 # direct effect of P on C
b_U <- 2  # direct effect of U on P and C

## R code 6.26
set.seed(1)
U <- 2*rbern( N , 0.5 ) - 1
G <- rnorm( N )
P <- rnorm( N , b_GP*G + b_U*U )
C <- rnorm( N , b_PC*P + b_GC*G + b_U*U )
d <- data.frame( C=C , P=P , G=G , U=U )
pairs(d, col = rangi2)

```

```{r}
## R code 6.27
# multiple regression model including Parent and grandparent education
m6.11 <- quap(
    alist(
        C ~ dnorm( mu , sigma ),
        mu <- a + b_PC*P + b_GC*G,
        a ~ dnorm( 0 , 1 ),
        c(b_PC,b_GC) ~ dnorm( 0 , 1 ),
        sigma ~ dexp( 1 )
    ), data=d )


## R code 6.28
# multiple regression model including P G education AND THE UNMEASURED variable
m6.12 <- quap(
    alist(
        C ~ dnorm( mu , sigma ),
        mu <- a + b_PC*P + b_GC*G + b_U*U,
        a ~ dnorm( 0 , 1 ),
        c(b_PC,b_GC,b_U) ~ dnorm( 0 , 1 ),
        sigma ~ dexp( 1 )
    ), data=d )


# visualize coefficients 
plot(precis(m6.11), main = 'm6.11 C ~ P + G')
plot(precis(m6.12), main = 'm 6.12 m6.11 C ~ P + G + U')
```

In m 6.11 it looks like grandparents negatively influence C but that was because we inadvertently conditioned on a hidden collider that we never measured. 

G causes P and U causes P. When we condition on P it introduces bias in the estimate of G -> C causing a negative correlation making it look like grandparent education negatively influences child education.  


Conditioning on P is like looking within P that have similar levels of education, once we know P, adding G to the model invisibly tells us about the unmeasured variable neighborhood U which is associated with the outcome C. The only way for 2 parents with equal education to have grandparents with very different education is for them to be from different neighborhoods (unmeasured).  


Blue cloud = good neighborhood. Black cloud = bad neighborhood. 



```{r}
suppressMessages(library(tidyverse))

d %>% 
  mutate(centile = ifelse(P >= quantile(P, prob = .45) & P <= quantile(P, prob = .60), "a", "b"),
         u = factor(U)) %>%
  ggplot(aes(x = G, y = C)) +
    theme_bw() + 
    geom_point(aes(shape = centile, color = u), size = 2.5, stroke = 1/4) +
    stat_smooth(data = . %>% filter(centile == "a"),
                method = "lm", se = F, size = 1/2, color = "black", fullrange = T) +
    scale_shape_manual(values = c(19, 1)) +
    scale_color_manual(values = c("black", "deepskyblue3")) +
    theme(legend.position = "none") + 
    ggtitle('filled points = parents in similar (45-65) percentile of education')

```

```{r, echo = FALSE}
suppressMessages(library(ggdag))

d1 <- 
  dagify(X ~ Z,
         Y ~ Z,
         coords = tibble(name = c("X", "Y", "Z"),
                         x = c(1, 3, 2),
                         y = c(2, 2, 1)))

d2 <- 
  dagify(Z ~ X,
         Y ~ Z,
         coords = tibble(name = c("X", "Y", "Z"),
                         x = c(1, 3, 2),
                         y = c(2, 1, 1.5)))

d3 <- 
  dagify(Z ~ X + Y,
         coords = tibble(name = c("X", "Y", "Z"),
                         x = c(1, 3, 2),
                         y = c(1, 1, 2)))

d4 <- 
  dagify(Z ~ X + Y,
         D ~ Z,
         coords = tibble(name = c("X", "Y", "Z", "D"),
                         x = c(1, 3, 2, 2),
                         y = c(1, 1, 2, 1.05)))

gg_simple_dag <- function(d) {
  
  d %>% 
    ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
    geom_dag_point(color = "steelblue", alpha = 1/2, size = 6.5) +
    geom_dag_text(color = "black") +
    geom_dag_edges() + 
    theme_dag()
  
}

p1 <- gg_simple_dag(d1) + labs(subtitle = "The Fork")
p2 <- gg_simple_dag(d2) + labs(subtitle = "The Pipe")
p3 <- gg_simple_dag(d3) + labs(subtitle = "The Collider")
p4 <- gg_simple_dag(d4) + labs(subtitle = "The Descendant")

```
```{r}
library(patchwork)
p1|p2|p3|p4
```


Rules for what varialbes to control for: 

1) Trace backwards starting at the predictor X ending att the outcome Y.  
2) Check for colliders (these are "closed paths")  
3) classify each path as a backdoor path or not (arrow entering a predictor)  
4) close all open backdoor paths.   

This is done algorithmically with `daggity` package and we get an output of the conditional independences given a causal dag. 

```{r}

## R code 6.29

dag_6.1 <- dagitty( "dag {
    U [unobserved]
    X -> Y
    X <- U <- A -> C -> Y
    U -> B <- C
}")
adjustmentSets( dag_6.1 , exposure="X" , outcome="Y" )


```
```{r}
## R code 6.30

dag_6.2 <- dagitty( "dag {
    A -> D
    A -> M -> D
    A <- S -> M
    S -> W -> D
}")
adjustmentSets( dag_6.2 , exposure="W" , outcome="D" )

## R code 6.31
impliedConditionalIndependencies( dag_6.2 )
```



problems  

6e1 false inference about causal effects can arise if we have a structural model connecting predictor variables to the outcome. Confounding can arise if we don't close a back door path between a predictor and outsome, if we condition on a collider or if we condition on a descendent that introduces a confound. We can also produce false inferences if we have the causal structure wrong to begin with (using multiple colinnear variables, giving us a weakly identifiable model  or any time we introduce a variable that is not actually causal).  

6e2  

Not having a randomized sample confounds inference. The idea with a randomized sample is that we close unobserved backdoor paths from the predictor variable of interest to the outcome of interest. Diet D, outcome O and unobserved U. 

Non randomized : 
```{r}
dag.ex <- dagitty( "dag {
    D -> O
    U -> O 
    U -> D
    
}")
drawdag(dag.ex)


```

Randomized: 
```{r}
dag.ex <- dagitty( "dag {
    D -> O
    U -> O 
    
}")
drawdag(dag.ex)
```

randmization blocks the backdoor path between the unobserved variable and diet. Conditioning has the same effect, however we cannot condition on unobserved variables. 

6e4  
the pipe, collider, the fork the descendent. 

```{r}

fork <- dagitty( "dag {
    x -> z -> y
}")
impliedConditionalIndependencies(fork)

```

```{r}
pipee <- dagitty( "dag {
    x <- z -> y
}")
impliedConditionalIndependencies(pipee)

```


```{r}
collider <- dagitty( "dag {
    x -> z <- y
}")
impliedConditionalIndependencies(collider)

```

6m1  

```{r}

dag = dagitty("dag{ 
              u [unobserved] 
              x -> y
              x <- u <- a -> c -> y
              u-> b <- c
              }")

dag2 = dagitty("dag{ 
              u[unobserved] 
              x -> y
              x <- u <- a -> c -> y
              u-> b <- c
              c <- v -> y
              }")

set.seed(140)
drawdag(dag); drawdag(dag2)
```

```{r}
adjustmentSets(x = dag,exposure = 'x', outcome = 'y')

```

this means either c orr a 

```{r}
adjustmentSets(x = dag2,exposure = 'x', outcome = 'y')
```

this means either c AND v or a. 

```{r}
impliedConditionalIndependencies(dag2)
```


see written out in  lecture 6 notes 

6m2  

Simulating data X -> Z -> Y  
Z and X are highly correlated  
include both in a prediction of Y. 


```{r}
N = 50
z = rnorm(n = N, mean = 0, sd = 0.2)
x = z + rnorm(n = N, mean = 0, sd = 0.1)
y = rnorm(n = N, mean = 2,sd = 2)
d = data.frame(x,z,y)
pairs(d, col = rangi2)

```


multiple regression model with two correlated predictors 

```{r}

f1 = alist( 
  # target 
  y ~ dnorm(mu, sigma),
  # likelihood
  mu <- alpha + (Bx*x) + (Bz*z),
  # priors 
  alpha ~ dnorm(0,1), 
  Bx ~ dnorm(0,1), 
  Bz ~ dnorm(0,1), 
  sigma ~ dexp(1)
  )

m1 = quap(flist = f1,data = d)
precis(m1)
```

This implies (similar to using both legs as predictors) that one effect is positive and the other negative, since x and z are both so highly correlated, there is a very narrow range of x values corresponding to any z value. 

*The parameters Bx and Bz never separately infuence y, only their sum influences y*  

We can't really get at mechanistic understanding. But for prediction, how does the multiple regression model do compared to the single regression model? 

```{r}
f1 = y ~ x + z 
f0 = y ~ x
m1 = lm(f1, data =  d)
m0 = lm(f0, data = d)
AIC(m1, m0)
```

Here the AIc is pretty similar so it doesn't really illustrate the point that the multi[le regression models often have better predictive power. 


6M3 see lec 6 written notes 


6h3  

FOXES ARE LIKE STREET GANGS 

```{r}
data("foxes")
d = foxes
head(d)
```


```{r}
set.seed(23)
foxdag = dagitty(
"dag{
A -> F -> W
F -> S
F -> W <- S
}")
drawdag(foxdag)
```

Total causal effect of area A on weight W 

```{r}
data("foxes")
d = foxes
d$S = d$groupsize = standardize(d$groupsize)
d$F = standardize(d$avgfood)
d$A = standardize(d$area)
d$W = standardize(d$weight)

d2 = d[ ,c('S','F','A','W')]
pairs(d2, col = rangi2)
```

from the pairs plot food size and area are pretty colinear and there is probably a complicated relationship with weight which is the target of inference. 

Total causal effect of area on weight. 

```{r}
adjustmentSets(x = foxdag,exposure = 'A', outcome = 'W')
```

prior predictive simulation. 
```{r}
f.fox = alist(
  W ~ dnorm(mu,sigma), 
  mu <- alpha + (BA*A), 
  alpha ~ dnorm(0, 0.5), 
  BA ~ dnorm(0, 0.5),
  sigma ~ dexp(1)
)
mfox = quap(flist = f.fox, data = d2)

# simulate prior from model fit 
p = extract.prior(mfox)

# range of values for standrdized area to simulate priors over 
aseq = c(-2,2)

# simulate posterior distribution of mu
prior.sim = link(fit = mfox, post = p, data = list(A = c(-2,2)))
plot(NULL, xlim = aseq, ylim = aseq)
for ( i in 1:100) lines(x = aseq, y = prior.sim[i, ], col = col.alpha(rangi2, 0.2))

```

posterior fit 
```{r}
precis(mfox)
```

plot posterior mean prediction and 89% compatibility interval vs data.

```{r}
sim.A = seq(-2, 2,length.out = 50)
# extract posterior distributions for 50 simulated x values 
post = link(mfox, data = data.frame(A = sim.A))

plot(W ~ A , d2, col = rangi2)
lines(sim.A, apply(post,2,mean))
shade(apply(post, 2, PI), sim.A)
```


There is very little association between area and weight. 


6H4  
total causal effect of adding food 
```{r}
adjustmentSets(x = foxdag,exposure = 'F', outcome = 'W')
```

don't need to adjust for anything. 


```{r}
f.fox.2 = alist(
  W ~ dnorm(mu,sigma), 
  mu <- alpha + (BF*F), 
  alpha ~ dnorm(0, 0.5), 
  BF ~ dnorm(0, 0.5),
  sigma ~ dexp(1)
)
mfox.2 = quap(flist = f.fox.2, data = d2)

precis(mfox.2)
```

similarly there is not really a direct effect of food on weight. 

6H5 total causal effect of group size 

```{r}
adjustmentSets(foxdag,exposure = 'S', outcome = 'W')

```

To estimate the total causal impact of group size we need to condition on F. 

```{r}

f.fox.3 = alist(
  W ~ dnorm(mu,sigma), 
  mu <- alpha + BF*F + BS*S, 
  alpha ~ dnorm(0, 0.5), 
  BF ~ dnorm(0, 0.5),
  BS ~ dnorm(0, 0.5),
  sigma ~ dexp(1)
)
mfox.3 = quap(flist = f.fox.3, data = d2)
precis(mfox.3)

```

The remaining effect of food on weight is positive, after adjusting for the the effect group size. 


```{r}
# hold area constant 
s.sim = c(-1,1)
d.test = data.frame(A = mean(d2$A), S = c(-1,1))
post.test <- link(mfox.3, data = d.test) 


mu.mean <- apply( post.test , 2 , mean )
mu.ci <- apply( post.test , 2 , PI ) 

plot( W ~ S, data=d2 , type="n" ) 
lines( s.sim , mu.mean ) 
lines( s.sim , mu.ci[1,] , lty=2 ) 
lines( s.sim , mu.ci[2,] , lty=2 )
```

Holding area constant, larger groups have lower weights prdicted by the model. 

```{r}
plot(d2$F, d2$A)
```

 Small groups with a large area and abundant food have are the ones with the highest predicted weights.
 



