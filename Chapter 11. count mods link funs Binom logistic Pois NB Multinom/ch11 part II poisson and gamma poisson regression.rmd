---
title: 'Ch 11 part II: Poisson and Gamma Poisson (NB) regression'
output: github_document
---

```{r, show = FALSE}
knitr::opts_chunk$set(
  #tidy = TRUE,
  #tidy.opts = list(width.cutoff = 95),
  message = FALSE,
  warning = TRUE, 
  eval = TRUE
  # root.dir = here()
)
```


```{r}
r = 'deepskyblue3'
suppressMessages(library(rethinking))
```


### Poisson regression
a binomial event with very small probability and large number of trials with an unknown upper bound. Expected value is Np. Variance is 1- Np which is approximately Np when N large p small. 

Simulating a binomial with very small p and large n is indeed very similar to scrnaseq counts. Same as globe toss -- we model 

```{r}
set.seed(10)
y = rbinom(n = 500, size = 100, prob = 1/1000)
y

```

Simulate many trials and compare mean and variance of the outcome. They are very similar 
```{r}
mean(y); var(y)
```

The poisson only has one parameter, lambda, its shape, which is the expected value of the outcome y. It's low. It's also the variance of y as shown above.  

With Poisson regression we use a log link function. the GLM takes the form: 

y[i] = Poisson(lambda[i])  
log(lambda[i]) = alpha + B(x[i])    

the purpose of the log link is to prevent the linear model from predicting a negative value for the number of events becaue that is not possible. It also assumes the parameters value is the exponential of the linear model:  

exp(log(lambda)) = lambda  
lambda = exp(alpha + B(x[i]))  

we need to be careful about the exponential relationship.  

### Kline data - oceana tool complexity  
The full dataset is below.   
```{r}

## R code 11.36
suppressMessages(library(rethinking))
data(Kline)
d <- Kline
d
```

hypothesis: number of unique tools increase as a function of log population size and contact rates and the impact of population on total tools is moderated by high contact rates -- an interaction effect.  

T[i] ~ Poisson(lambda[i])  
log(lambda[i]) = a[contactID] + B[contactID]*log(P[i])  
~then set priors for alpha[j] and beta[j]~  

Standardize variables. 
```{r}
## R code 11.37
d$P <- scale( log(d$population) )
d$contact_id <- ifelse( d$contact=="high" , 2 , 1 )
```

For sake of comparison with a frequentist poisson glm:  
```{r}
d$contact = factor(d$contact, levels = c('low', 'high'))
m1 = glm(formula = total_tools ~ P + contact + P*contact, 
         family = poisson(link = "log"),
         data = d)
precis(m1)

```

```{r}
emmeans::emmeans(m1,specs = ~contact)
```


```{r}
exp(m1$coefficients)
```

Back to the bayesian model: setting priors for alpha and beta helps understand how the model works. Transformation from the link function means that scale of linear model and count cale of outcome == something flat on linear model scale will not be flat on outcome scale.  

An intercept only model:  
T[i] = Poisson(lambda[i])  
log(lambda[i]) = alpha  
alpha = N(0,10)  

What does the flat prior on alpha imply?  
In other words, what does the prior look like on the outcome scale lambda?  Alpha is normally distributed so lambda is log normally disributed 
```{r}
## R code 11.38
curve( dlnorm( x , 0 , 10 ) , from=0 , to=100 , n=200 )
```

The mean is at 0 that is fine but what does the long tail imply about possible values for the mean number of unique tools?
```{r}

## R code 11.39
a <- rnorm(1e4,0,10)
# log(lambda) = a
lambda <- exp(a)
mean( lambda )
```
 too many tools. 
 
### Intuition for the log link function  
The log link puts all numbers from -inf to 0 between 0, 1 on the outcome scale. If we use a 0-centered prior, with a log normal outcome we will put half of the probability mass below 0, *half* of the prior mass on the outcome scale lambda will be between 0 and 1. Below we compare this to a prior on alpha of N(3, 0.5). 
 
```{r}

curve( dlnorm( x , 0 , 10 ) , from=0 , to=100 , n=200 )
curve( dlnorm( x , 3 , 0.5 ) , from=0 , to=100 , n=200,
       add = TRUE, col = r)

```
 
The same thing applies to the prior on beta in the interaction model.  
Below we simulate 100 trends for unique tools ~ log population based on the interaction model, using a flat prior on beta or using a weakly informative regularizing pror that does not put half of the probability mass between 0 and 1. 
This model in black predicts either explosive growth of tools just before the population mean or catastrohpic decline in number of tools just above the population mean. This is an insane prior.  

We can also look at a regularizing prior in blue. 

```{r}
## R code 11.41
N <- 100 # number of simulations 
a <- rnorm( N , 3 , 0.5 ) # alpha prior 
b <- rnorm( N , 0 , 10 ) # flat beta prior 
b2 <- rnorm( N , 0 , 0.2 ) # flat beta prior 

par(mfrow = c(1,2))
plot( NULL , xlim=c(-2,2) , ylim=c(0,100), 
      xlab = 'standardized log population', 
      ylab = 'number of unique tools', main = "beta N(0,10) prior" )
for ( i in 1:N ){ 
  curve( exp( a[i] + b[i]*x ) , add=TRUE , col=grau())
}

plot( NULL , xlim=c(-2,2) , ylim=c(0,100), 
      xlab = 'standardized log population', 
      ylab = 'number of unique tools', main = "beta N(0,0.3) prior" )
for ( i in 1:N ){ 
  curve( exp( a[i] + b2[i]*x ) , add=TRUE , col=col.alpha(r, 0.3))
}

```

### understanding different scales of the predictor variable  

Below we look at simulation on the standardized and unstandardized population scale we also use a regularizing prior on beta. In the left plot the unstandardized log population size is used. Then on the right we exponentiate the predictor x (population) put it back on the raw population scale.  

model:  
log(lambda) = a + bx  
lambda = exp( a + bx )  
b as log population  
then b as exp(log(population)) puts population on the raw scale.  


```{r}
# use a regularizing prir on beta 
b <- rnorm( N , 0, 0.2)

# simulate x (population) values on the log scale 
x_seq <- seq( from=log(100) , to=log(200000) , length.out=100 )

# compute predicted values for lambda - n unique Tools predicted by model for each populaiton value 
lambda <- sapply( x_seq , function(x) exp( a + b*x ) )

# plot 
par(mfrow = c(1,2))
plot( NULL , xlim=range(x_seq) , ylim=c(0,500) , 
      xlab="log population" , ylab="total tools")
for ( i in 1:N ){
  lines( x_seq , lambda[i,] , col=col.alpha(r, 0.5) , lwd=1.5 )
}

## R code 11.44
# on the natural population scale 
plot( NULL , xlim=range(exp(x_seq)) , ylim=c(0,500) , xlab="population" ,
    ylab="total tools" )
for ( i in 1:N ) lines( exp(x_seq) , lambda[i,] , col=col.alpha(r, 0.5) , lwd=1.5 )



```


The natural population scale curves bend in the other direction. This happens when we put a log predictor variable inside a linear model. Poisson models with a log link make **log linear** relationships with the predictor variables, i.e. the relationships implied by te model as shown on the left would be linear if we logged lambda as implied by log(lambda) = a + Bx. When one of the predictor variables is itself logged, that assumes diminishing returns for the raw variable as seen on the right.  

This diminishing returns phemoninon is a good reason to use log transformed predictor variables. 


### Fitting the models 

Now we fit the models both a intercept only and a interaction model. Remember -- cid means contact id its a binary high low variable. 
```{r}

# Format data for stan
dat <- list( 
  T = d$total_tools ,
  P = d$P ,
  cid = d$contact_id 
  )

# intercept only model 
f11.9 = alist(
  T ~ dpois(lambda), 
  log(lambda) <- a, 
  a ~ dnorm(3, 0.5)
)

# interaction model
f11.10 = alist(
  T ~ dpois(lambda), 
  log(lambda) <- a[cid] + b[cid]*P, 
  a[cid] ~ dnorm( 3 , 0.5 ),
  b[cid] ~ dnorm( 0 , 0.2 )
)

```

Now fit the models with stan MCMC

```{r, results = 'hide'}
m11.9 <- ulam(f11.9, data=dat , chains=4 , log_lik=TRUE )
m11.10 <- ulam( f11.10, data=dat , chains=4 , log_lik=TRUE )

## R code 11.46
compare( m11.9 , m11.10 , func=PSIS )
```

The column `pPSIS` is the number of effective parameters -- remember this is a not so great name for the overfitting penalty. In simple linear regression with flat priors th number of parameters we add to the model increases over fitting. In other models, for example bounded models such as this poisson model, there is not a clear relationship because parameter values near the boundary produce less overfitting than those near the boundary.  


Quick comparison to the simple glm fit. 

The parameters `contactlow` and `contacthigh` are analagous to the intercept terms inexed over contact levels from the bayesian model `a[cid]`. The beta for population is `P` which is `b[1]` in the bayesian model and the interaction effect is `p:contact` which is `b[2]` in the bayesian model. The bayesian model has a bit higher interaction effect average but the overall estimate is highly uncertain. 
```{r}
m1 = glm(formula = total_tools ~ 0 + P + contact + P*contact,
         family = poisson(link = "log"),
         data = d)
precis(m1)

```
```{r}
precis(m11.10, depth = 2)
```

Plot the raw data with points labeled by contact rates open points for low contact societies, size for pareto k values. 

```{r}
## R code 11.47
k <- PSIS( m11.10 , pointwise=TRUE )$k
plot(dat$P , dat$T,  
     xlab="log population (std)" , ylab="total tools" ,
     col=rangi2 , 
     pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
     ylim=c(0,75) , cex=1+normalize(k) 
     )
```

now superompise posterior predictions 

```{r}

plot(dat$P , dat$T,  
     xlab="log population (std)" , ylab="total tools" ,
     col=rangi2 , 
     pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
     ylim=c(0,75) , cex=1+normalize(k) 
     )
# set up the horizontal axis values to compute predictions at
ns <- 100
P_seq <- seq(from = -1.4 ,to = 3, length.out = ns)

# predictions for cid=1 (low contact)
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE , col =col.alpha('blue', 0.2))

# predictions for cid=2 (high contact)
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=1 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE , col = col.alpha('red', 0.2))
```

It is very helpful to see how highly leveraged the point (Hawaii) is on Now the raw scale. Vlsualize this by computing predictions over a differeent population sequence: 

```{r}
ns <- 100
P_seq <- seq( from=-5 , to=3 , length.out=ns )
# 1.53 is sd of log(population)
# 9 is mean of log(population)
pop_seq <- exp( P_seq*1.53 + 9 )
plot(pop_seq, col = r)
```


```{r}
## R code 11.48
plot( d$population , d$total_tools , xlab="population" , ylab="total tools" ,
    col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
    ylim=c(0,75) , cex=1+normalize(k) )

ns <- 100
P_seq <- seq( from=-5 , to=3 , length.out=ns )
# 1.53 is sd of log(population)
# 9 is mean of log(population)
pop_seq <- exp( P_seq*1.53 + 9 )

lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( pop_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , pop_seq , xpd=TRUE )

lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( pop_seq , lmu , lty=1 , lwd=1.5 )
shade( lci , pop_seq , xpd=TRUE )
```


### Replacing a linear model with a dynamical model  

innovation adds tools to a population  
proceses of loss remove them  
innovation is proportional to populaiton size with some diminishing returns  
Change in number of tools in some time step  
∆T = tool production - tool loss  
∆T = alphaP^beta - gamma(T)  
As in stochastic simulations we solve for the equilibrium number of tools by setting ∆T = 0  
T = alphaP^beta / gamma  

Now we have a scientific function for T, we can **embed this in a glm**. 
The noise around the outcome T is still Poisson distributed because it is the maximum entropy distribution for finite counts with no clear upper bound. (analagous to the 'error model').

T[i] = Poisson( lambda[i] )  
lambda[i] = alphaP^beta / gamma  

There is no linear model or link function. 


```{r, results = 'hide'}
## R code 11.49
dat2 <- list( T=d$total_tools, P=d$population, cid=d$contact_id )
f11.11 = alist(
  # poisson noise around the outcome variable:
  T ~ dpois( lambda ),
  # the dynamical model for the rate parameter:
  lambda <- exp( a[cid] )*P^b[cid]/g,
  
  # priors need to be positive.
  a[cid] ~ dnorm(1,1),
  b[cid] ~ dexp(1),
  g ~ dexp(1)
  )
m11.11 <- ulam(f11.11, data=dat2 , chains=4 , log_lik=TRUE )
precis(m11.11, depth = 2)
```

### Gamma Poisson (Negative Binomial)  
NB is a mixture of Poissons similar to how the students-t is a mixture of normals. 
help explain unexplained variation in the rate parameter.  

See written notes on offset and exposure. *lambda is a rate* realizing that helps intuition behind the offset because we can write lambda[i] as µ[i] / tau[i]. This lets us write the linear models as:  
log(µi) = log(tau[i]) + a + bx -- see written notes. What this translate to is we can model cases with different **exposures** (denominator in the rate) by incorporating log(tau) into the linear model as a parameter for the exposure, i.e. log(number of UMIs) representing the exposure for a single cell.  

Based on the equations this is always log( exposure ).

Simulating manuscript copyign example with a true rate of 1.5 manuscripts copied per day. 
```{r}
## R code 11.50
num_days <- 30
y <- rpois( num_days , 1.5 )

```

Now we have a norther monestary that copies 0.5 per day but we only have data on number per month. 
```{r}
## R code 11.51
num_weeks <- 4
y_new <- rpois( num_weeks , 0.5*7 )

```

Now we have days as an offset variable in the model 

```{r}
## R code 11.52
y_all <- c( y , y_new )
exposure <- c( rep(1,30) , rep(7,4) )
monastery <- c( rep(0,30) , rep(1,4) )
d <- data.frame( y=y_all , days=exposure , monastery=monastery )
d
```

Now we fit a glm to estimate the rate at each monestary adding the log of the exposure. 

```{r}
## R code 11.53
# compute the offset
d$log_days <- log( d$days )

f11.12 = alist(
  y ~ dpois( lambda ), # estimand
  
  # log lambda = offset + intercept + slope
  log(lambda) <- log_days + a + b*monastery,
  
  # priors       
  a ~ dnorm( 0 , 1 ),
  b ~ dnorm( 0 , 1 )
) 

# fit the model
m11.12 <- quap(flist = f11.12, data=d)
precis(m11.12)
```

So the second monestary effect was 1 less per day which is what we simulated (a true rate of 0.5 per day vs 1.5 per day).  

When we do predictions we don't incorporate the offset becuse the model posteriors are already in terms of days. 

```{r}
## R code 11.54
post <- extract.samples( m11.12 )
lambda_old <- exp( post$a )
lambda_new <- exp( post$a + post$b )
precis( data.frame( lambda_old , lambda_new ) )
```
