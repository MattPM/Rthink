---
title: 'Ch 11 p set'
output: github_document
---


```{r}
suppressMessages(library(rethinking))
suppressMessages(library(magrittr))
```


11E1 - log odds of pr = 0.35 
```{r}
log(0.35  / (1- 0.35))
```

11E2 - log odds of event = 3.2 what is p 

```{r}

inv_logit(3.2)

```

to get a feel for log odds vs p:  
at 50% p, log odds is 0 at 0.1 its around -2 and at 0.9 its around 2. 
```{r, fig.width=3, fig.height=3}
pvec = seq(0, 1, by = 0.1)
plot(pvec, log(pvec / (1-pvec)))
```


11E3 logistic regression beta = 1.7 what does it say about proportional change in odds of the event?  

logistic regression coefs are in units of log odds, so 1.7 is log odds. The odds is the exponentiated coefficient (transforms to the outcome scale)

```{r}
exp(1.7)
```

11E4  
Poisson regression requires an offset because the totla counts per class (exposure) can be different, i.e. sampling one hospital for events for a year and another for 2 years. Or in scRNAseq the rate parameter for a gene tells you something but to conrast, the total genes per cell (log UMI) needs to be accounted for. 

11M1  
the likelihood is the probability of the data given the model so in the case of aggregated binomial globe toss, 3 water one land, that is 3 W in 4 trials, by embedding this into the binomial probability distribution we are aggregating the the data, the multiplicity term wiht n choose k * 1-k is multiplying the number of ways to observe, ie.  W L W W | W W W L | W W L W etc. we then plug in the probability. For individual counts, we calculate p * (1- p) for each separately. Basically p is independent of the likeihood function. 

11M2  

Poisson regression coefficient 1.7:  

y[i] = Poisson(lambda[i])   
log(lambda[i]) = a + Bx[i]  

the a + bX term is equal to log(lambda)  
```{r}
exp(1.7)
```

Fr every 1 unit change in the predictor there is a 5.47 unit change in the outcome variable. *note, in looking up the correct answer to this, there was a slightly more nuanced answerusing ratio of means exp( a + Bx + 1) / exp(a + Bx); the top part of the fraction is the 1 unit of change in predictor, solving you get exp(B), as i calculated above. but important to note that the interpretation of this is the proportional change in the expected count very similar to proportional change in log odds from logistic regression*  

11M3 explain why logit link appropriate for binomial GLM.    

Because binomial is mapping the linar model output which is 0 , inf onto 2 possible outcomes, yes, no, heads, tails, 1, 0 etc. The odds makes sense to use because we can write probability functions in terms of one outcome like heads then the odds (logit) captures p ( 1-p ).  

11M4 explain why log link appropriate for Poisson.  

The log link constrains predictions from the linear model onto a space that will not predict negative counts. When we get a prediction from the linear model:   

log(lambda) = a + Bx  

then lambda = exp(a + Bx)  
exp is raising e to the power of the linear model, so say a+ Bx evaluated to -5, the log link will make lambda at i =   
```{r}
exp(-5)
```
meaning basically 
```{r}
2.71^-5
```


*It is conventional to use a logit link for a binomial GLM because we need to map the continuous linear model value to a probability parameter that is bounded between zero and one.*  

*It is conventional to use a log link for a Poisson GLM because we need to map the continuous linear model value to a mean that must be positive.*  

11M5 could it make sense to use a logit link with a poisson GLM?  
using a logit link makes the outcome bounded between 0 and 1.  
basically if you want to put a upper bound on the estimates. see manual ed 1 p73. One could potentially do this -- a cell does have an upper bound as to what the total counts could be, so we would expect the counts of individual genes to be some fraction of that total; we'd have to define this for eahc cell type and its more a function of the technology used.  

M116 State constraints under which poisson and binomial have maximum entropy  

The binomial distribution has maximum entropy for a binary outcome with a constant expected rate, for example a globe toss with expected probability of water 0.7 what is the likelihood of observing 5 waters in a row -- the likeliood function with the least assumptions is the one that can happen the most ways which is the binomial.  

A Poisson distribution has maximum entropy under the constraint that there is a constant expected rate, as the binomial with low probability of observing a "success" (count) in each trial. its a generalization of the binomial.  

11M8 Kline data without hawaii- fit an interaction model  

likelihood for interaction model with a category and continuous var:  
y = a[group] + B[group] * predictor  

embed that in the glm using the link function on outcome  
log( y ) = ...  

```{r, results = 'hide'}
data("Kline")
d = Kline
d = d[!d$culture == 'Hawaii', ]
d$pop = log(d$population)


# fit mcmc binomial model 
f1.1 = alist(
  # estimand
  total_tools ~ dpois(lambda), 
  # likelihood
  log(lambda) <- a[cid] + B[cid]*pop, 
  # priors 
  a[cid] ~ dnorm(3, 0.3), 
  B[cid] ~ dnorm(0, 0.1)
)

# specify data for stan
dat = list(
  total_tools = d$total_tools, 
  pop = d$pop, 
  cid = d$contact
)

# fit model 
m1.1 = ulam(flist = f1.1, data = dat,cores = 4)

# coefficients
precis(m1.1, depth = 2)

```


Important -- the model converts the 'high', 'low' factor to indicators for contact ID; this screwed up the ability to use 'high low' in the step idx = which(m1.1@data$cid ==ct). So probably best to use the data that is stored in the stan model fit when using link. 

```{r}
suppressMessages(library(tidyverse))

# extract posterior 
post = extract.samples(m1.1)
m1.1@data
pop.seq = seq(min(d$pop), max(d$pop), length.out = 45)
pop.lim = c(min(d$pop), max(d$pop))
tool.seq = c(min(d$total_tools), max(d$total_tools))
## 

par(mfrow=c(1,2))
for (ct in c(1,2)) {
  
  idx = which(m1.1@data$cid ==ct)
  
  # compute the posterior for the levels of contact ID 
  mu = link(m1.1, data = data.frame(cid = ct, pop = pop.seq))
  plot( d$pop[idx], d$total_tools[idx], xlim = pop.lim, ylim = tool.seq)
  for (i in 1:20) {
    lines(x = pop.seq, y = mu[i, ], col = col.alpha('grey', 0.4))
  }
}


```

another way to visualize the interaction 
```{r}

pop.seq = c(min(d$pop), max(d$pop))
tool.seq = c(min(d$total_tools), max(d$total_tools))

mu1 = link(m1.1,data = data.frame(cid = 1 , pop = pop.seq))
mu1.mean = apply(mu1, 2, mean)
mu1.pi = apply(mu1, 2, PI)

mu2 = link(m1.1,data = data.frame(cid = 2 , pop = pop.seq))
mu2.mean = apply(mu2, 2, mean)
mu2.pi = apply(mu2, 2, PI)

plot(dat$pop, dat$total_tools, col = rangi2, pch = 16)
lines(pop.seq, mu1.mean, col = 'grey')
shade(mu1.pi, lim = pop.lim)
lines(pop.seq, mu2.mean, col = 'black')
shade(mu2.pi, lim = pop.lim)
```



compare to frequentist glm fit 


```{r}
# fit frequentist binomial glm with log link

d$con.id = ifelse(d$contact == 'low', 0, 1) 
f1 = total_tools ~  pop + con.id + pop:con.id
m2 = glm(formula = f1, family = poisson(link = 'log'),data = d)
precis(m2)
```


Interpretation of coefficients:   
(intercept) - Y intercept for the baseline reference group 'low contact'  
1.32  
b1 = pop = slope or the baseline group  
0.22  

regression equation for low contact reference group  
y = 1.32 + (0.22 * xi)  

b2 = conid = offset for the y intercepy for the alternate group 'high contact'  
-0.69  
b3 = offset to the slope of the alternative group compared to the reference group high vs low  
0.11  

regression equation for the low contact group  
y = (1.32 +  -0.69 ) + (0.11 + 0.22 * xi)   


```{r}
precis(m1.1, depth = 2)
```



```{r}
par(mfrow = c(2,2))

plot(
  exp(predict(m2, newdata = data.frame(pop =  d$pop, con.id = 1))), 
  pch = 16, col = 'blue')
plot(
  exp(predict(m2, newdata = data.frame(pop =  d$pop, con.id = 0))), 
  pch = 16, col = 'blue')

plot(
  exp(mean(post$a[,1]) + mean(post$B[ ,1])*dat$pop), 
  pch = 16, col = rangi2
  )

plot(
  exp(mean(post$a[,2]) + mean(post$B[ ,2])*dat$pop), 
  pch = 16, col = rangi2
  )

```

11H3  

was struggling to get ulam to fit this prob bc of bad priors. 

```{r}
suppressMessages(library(rethinking))
suppressMessages(library(tidyverse))

data(salamanders)
d = salamanders

d$s = d$SALAMAN
d$p = (d$PCTCOVER - mean(d$PCTCOVER) ) / sd(d$PCTCOVER)
d$a = (d$FORESTAGE - mean(d$FORESTAGE)) / sd(d$FORESTAGE)

dlist = list( 
  sal = d$s, 
  pct = d$p,
  age = d$a
  )

pairs(dlist, col = rangi2, pch = 16)

```

it looks like old growth forests have increased ground cover based on pct ~ age. Once the forest hits a certain percentage the salmon population can take off (it becomes more like gaussian dist).

There is a direct causal effect maybe of forest age on salamander, but also an effect mediated through percent coverage. 



```{r, results = 'hide'}

f1 <- alist(
  sal ~ dpois(lambda), 
  log(lambda) <- a + b*pct,
  b ~ dnorm(0, 1),
  a ~ dnorm(0, sd = 1)
)

m1 <- ulam(flist = f1, data = dlist, chains = 4, cores = 4)
trankplot(m1)
```


```{r}
precis(m1)
```

```{r}

# extract parameters 
post <- extract.samples(m1, n = 50)
head(post)
```


```{r}

xseq = seq(-2,2, length.out = 30)
# summarize estimand based on new predictor values
mu = link(m1, n = 100, data = data.frame(pct = xseq))
mu.mean = apply(mu, 2, mean)
mu.PI = apply(mu, 2, PI)

# simulate uncertainty in estimates based on full model 
dsim = sim(fit = m1, data = data.frame(pct = xseq))
sim.pi = apply(dsim, 2, PI)

# plot summary
plot(sal ~ pct, dlist, col = rangi2, pch = 16)
lines(x = xseq,y = mu.mean, lwd = 3) # mean
shade(object = mu.PI, lim = xseq) # mean uncertainty
shade(object = sim.pi,lim = xseq)


```


Given the pairs plot the model `log(lambda) <- a + b*pct + b2*age` woudl prob predict better but would be harder to understand the estimates. Same prediction vs interpretability trade off but here we can see this is because the 2 predictors are highly **collinear**. There is a fundamental different scientific process for undertanding the links between variables and doing predictive modeling. 







