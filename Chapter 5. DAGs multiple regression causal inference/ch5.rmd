---
title: 'Chapter 5 The Many Variables & The Spurious Waffles'
output: github_document
---

<!--   output: -->
<!--   html_document: -->
<!--     df_print: paged -->

<!-- editor_options: -->
<!--   chunk_output_type: inline -->

### causal inference, directed acyclic graphs, multiple regression, confounds  

### 5.1 Spurious Association  

```{r}
# load data and copy 
suppressMessages(library(rethinking)) 
suppressMessages(library(tidyverse)) 
data(WaffleDivorce) 
d <- WaffleDivorce

# standardize variables
d$D <- standardize( d$Divorce )
d$M <- standardize( d$Marriage )
d$A <- standardize( d$MedianAgeMarriage )

```

The linear model:  
D[i] ∼ Normal( µ i , σ)  
µ[i] = α + βA Ai  
α ∼ Normal(0, 0.2)  
βA ∼ Normal(0, 0.5)  
σ ∼ Exponential(1)  

Intuition on working with the priors with data in scaled space: 

If a prior on β was 1, 1 sd change in age would lead to 1sd change in divorce. What is 1sd in age? 
```{r}
sd(d$MedianAgeMarriage)
```

So with a prior on β of 1, 1.24 years delta would lead to 1sd delta in divorce. That is too strong of an effect. 

Model in quap for both variables  
```{r}
m5.1 = quap(
  flist = alist(
    D ~ dnorm(mu, sigma),
    # bA is the beta for age (bA * A is βA)
    # A is standardized age as above
    mu <- a + bA * A, 
    a ~ dnorm(0, 0.2),
    # prior for the age effect 
    bA ~ dnorm(0,0.2), 
    sigma ~ dexp(1)
    ), data = d )


## R code 5.6
m5.2 <- quap(
    alist(
        D ~ dnorm( mu , sigma ) ,
        mu <- a + bM * M ,
        a ~ dnorm( 0 , 0.2 ) ,
        bM ~ dnorm( 0 , 0.5 ) ,
        sigma ~ dexp( 1 )
    ) , data = d )



```

Simulate from the priors with extract.prior and link 

```{r}
## R code 5.4
set.seed(10)
prior <- extract.prior(m5.1)
mu <- link(m5.1, post = prior, data = list(A = c(-2,2) )) 
plot( NULL , xlim=c(-2,2) , ylim=c(-2,2) )
for ( i in 1:50 ) lines( c(-2,2) , mu[i,] , col=col.alpha("black",0.4) )

```

Posterior predictions from the model  

```{r}
## R code 5.5
# compute percentile interval of mean
A_seq <- seq( from= -3 , to=3.2 , length.out=30 )
mu <- link(m5.1 , data=list(A=A_seq))
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply( mu , 2 , PI )

# plot the actual data 
plot(D ~ A, data=d , col=rangi2)
# plot the average value for the line from the posterior of lines 
lines(x = A_seq, y = mu.mean, lwd=2 )
# plot the percentile interval 
shade(mu.PI , A_seq)
```



DAGs - see in depth lecture notes. 

DAGs provide a way to decide on which variables to condition estimates on in a principled way, provided one has a scientific model about the way that the variables relate to one another.  

Most of the examples introduced in ch 5,6 have time-dependence in their relationships. How do we know to draw the directions of the arrows like this? 
```{r}
## R code 5.7
library(dagitty)
dag5.1 <- dagitty( "dag{ A -> D; A -> M; M -> D }" )
coordinates(dag5.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )
drawdag( dag5.1 )
```

A - Age at marriage  
M - marriage rate  
D - divorce rate  

Here the arrow of time dictates the possible causal paths, to some degree this is possible in systems level datasets, if T = transcription of some gene  genotype -> T  and not T -> genotype, this is the basis of the intermediate phenotype concept of eQTL studies. 

Time-dependence dictates the relationships reason we know how to draw arrows from A to M and D and from M to D.  
A -> D ; have to be married before D, it could influence because younger people may change at a faster rate.  
A -> M -> D ; indirect effect by lowering age at marriage, therate icreases b/c there are more young people who haven't died to get married.  


**Conditional independencies are testable implications** 
Any DAG may imply some variables are independent of others under certain conditions.
These are the model’s testable implications/ conditional independencies, come in 2 forms:  
1) Statements of which variables should be associated with one another (or not) in the data. 
2) Statements of which variables become dis-associated when we condition on some other set of variables.

We can algorithmically find the conditional independencies implied by the model, for example the 
```{r}
DMA_dag2 <- dagitty('dag{ D <- A -> M }') 
drawdag(DMA_dag2)
impliedConditionalIndependencies( DMA_dag2 )

```

Read this as "D is independent of M, Conditioned on A. 
This is for model 2 where 

```{r}
DMA_dag1 <- dagitty('dag{ D <- A -> M -> D }') 
drawdag(DMA_dag1)
impliedConditionalIndependencies( DMA_dag1 )
```

There is no output:  

The testable implications of DMA_dag1 are that all pairs of variables should be associated, whatever we condition on.  
The testable implications of DMA_dag1 are that all pairs of variables should be associated, before conditioning on anything, but also that S is independent on M after condititioning on A.  

A multiple regression model will condition on A so the above gives us a principled way to decide to condition on A.  


The multiple regression model asks:  
If we already know marriage rate, what additional value does knowing age at marriage add?  
If we already know age at marriage, what additional value does knowing marriage rate add?    


### 5.1.3 (p135) Multiple regression notation  

The mean of the outcome variable is a modeled as the sum of  intercept+ an additive combination of products of parameters (betas) and predictors. Since the predictors have different values, the mean will again depend on i.  

µi = α + βmMi + βaAi  
*Note p 136 box on forms of the notation*  

D[i] ∼ Normal(µi, σ)  -- the outcome D is distributed normally with mean µ dependent of row i with sd sigma.  
µ[i] = α + βM * M[i] + βA * Ai -- the mean oud outdcome depends on the row and is a combination of effect of A and M (i.e. µ[i] = α + βx1 + βx2)  
α ∼ Normal(0, 0.2) -- The prior for the intercept is normally distributed   
βM ∼ Normal(0, 0.5) -- prior for the effect of M is normally distributed   
βA ∼ Normal(0, 0.5) -- prior for the effect of A is normally distributed  
σ ∼ Exponential(1) -- prior for standard deviation of D is normal  

Quap code:  

```{r}
## R code 5.10
m5.3 <- quap(
    alist(
        D ~ dnorm( mu , sigma ),
        mu <- a + bM*M + bA*A, # deterministic component of model, use assignment operator <- 
        a ~ dnorm( 0 , 0.2 ),
        bM ~ dnorm( 0 , 0.5 ),
        bA ~ dnorm( 0 , 0.5 ),
        sigma ~ dexp( 1)
    ) , data = d )
precis( m5.3 )
```


m5.1: D ~ A   
m5.2 D ~ M  
m5.3 D ~ M+A  
```{r}
plot(
  coeftab(m5.1, m5.2, m5.3), 
  par=c("bA","bM")
  )
```
  
 Interpretation of the above: M is only associated with D when Age is not in the model.  Once we know age at marriage A, there is no additional predictive power in knowin rate of marriage M.  
 
From a predictive standpoint M adds value to predicting D if you don't know A, but this model is telling us *because we have the DAG*, that the association M -> D  is a spurious relationship **that is caused by the influence of Age on both Marriage rate and Divorce**.  

Parameter estimate interpretation always depends on the DAG / causal model. Often on set of parameter values are consistent with many causal models = **markov equivalence**.  


Visualizing the multiple regression posteriors to get a feel for teh influence of the conditional independences.  

(1) Predictor residual plots: outcome ~ residual(m.1) help understand the model.  

(2) Posterior prediction plots: model-based predictions vs raw data. useful for checking fit / predictions.  

(3) Counterfactual plots: implied predictions for imaginary experiments. Explore causal implications of manipulating variables.  



5.1.5.1  predictor residual plots  

Average prediction error when we use all other predictor variables to model a *predictor* of interest. So we make a model of M here using A.  

```{r}
## R code 5.13
m5.4 <- quap(
    alist(
        M ~ dnorm( mu , sigma ) ,
        mu <- a + bAM * A ,
        a ~ dnorm( 0 , 0.2 ) ,
        bAM ~ dnorm( 0 , 0.5 ) ,
        sigma ~ dexp( 1 )
    ) , data = d )

## R code 5.14
mu <- link(m5.4)
mu_mean <- apply( mu , 2 , mean )

# compute the residual 
# subtracting  observed marriage rate from the model predicted rate 
# based on the model above
mu_resid <- d$M - mu_mean

## R code 5.15
# call link without specifying new data
# so it uses original data
mu <- link( m5.3 )

# summarize samples across cases
mu_mean <- apply( mu , 2 , mean )
mu_PI <- apply( mu , 2 , PI )

# simulate observations
# again no new data, so uses original data
D_sim <- sim( m5.3 , n=1e4 )
D_PI <- apply( D_sim , 2 , PI )

## R code 5.16
plot( mu_mean ~ d$D , col=rangi2 , ylim=range(mu_PI) ,
    xlab="Observed divorce" , ylab="Predicted divorce" )
abline( a=0 , b=1 , lty=2 )
for ( i in 1:nrow(d) ) lines( rep(d$D[i],2) , mu_PI[,i] , col=rangi2 )
```

Plotting the residuals against the outcome:  

```{r}
plot(d$D ~ mu_resid, col = rangi2, xlab = 'marriage rate residual')
```
The plot above shows the linear relationship between divorce and marriage rates, having “controlled” for median age of marriage.  
Average divorce rate on both sides of 0 is about the same there is little relationship between divorce and marriage rates conditional on A.  


multiple regression measures the *remaining* association *of each predictor* with the outcome, *after already knowing the other predictors.*  

